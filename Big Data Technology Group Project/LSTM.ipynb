{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BVsGQZ-PLErh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SbN0lTv8NP_O"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "large_classification_data = pd.read_csv(\"CDs_and_Vintl_sentiment.csv\")\n",
    "large_classification_data.index = range(len(large_classification_data))\n",
    "\n",
    "## drop unmeanningful reviews\n",
    "new_large_classification_data=large_classification_data.drop(labels=[1917,43407, 62315,64996,68579,74976,82369,\n",
    "                                                             90049,98337,101219,139591,130787,267578,\n",
    "                                                             295804,335877,396767,159574,164137,223911,\n",
    "                                                             254229,260236,261849,271662,287960,288971,\n",
    "                                                             374860,378999,380784,383545],axis=0)\n",
    "new_small_classification_data = new_large_classification_data.sample(10000, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "XIboy6MNMEfn"
   },
   "outputs": [],
   "source": [
    "## Make Dictionary\n",
    "reviews = (\"\\n\".join(i for i in new_large_classification_data['text'].astype(str)))\n",
    "reviews_split = ''.join([c for c in reviews.lower() if c not in punctuation]).split('\\n')\n",
    "all_text = ' '.join(reviews_split)\n",
    "words = all_text.split()\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "## Embedding the words\n",
    "reviews_ints = []\n",
    "for review in reviews_split:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()])\n",
    "\n",
    "## Padding\n",
    "MAX_LEN=100\n",
    "reviews_ints=np.array(pad_sequences(reviews_ints, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\"))\n",
    "\n",
    "## Dividing Training and Testing\n",
    "train_x, val_x, train_y, val_y = train_test_split(reviews_ints, np.array(new_large_classification_data['label']), random_state=2018, test_size=0.1)\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "batch_size = 30\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "train_on_gpu=torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "VfdJf6zGQBwV"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "TdqxEe7VQNHL"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 200\n",
    "hidden_dim = 128\n",
    "n_layers = 3\n",
    "\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "m84yjTbtMker",
    "outputId": "bd37e8d5-df5f-423d-f6f9-2e5a8344fc06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2... Step: 100... Loss: 0.690162... Val Loss: 0.691499\n",
      "Epoch: 1/2... Step: 200... Loss: 0.699965... Val Loss: 0.689950\n",
      "Epoch: 1/2... Step: 300... Loss: 0.723616... Val Loss: 0.691549\n",
      "Epoch: 1/2... Step: 400... Loss: 0.705618... Val Loss: 0.688439\n",
      "Epoch: 1/2... Step: 500... Loss: 0.693296... Val Loss: 0.700096\n",
      "Epoch: 1/2... Step: 600... Loss: 0.726101... Val Loss: 0.680286\n",
      "Epoch: 1/2... Step: 700... Loss: 0.728043... Val Loss: 0.679064\n",
      "Epoch: 1/2... Step: 800... Loss: 0.662923... Val Loss: 0.681812\n",
      "Epoch: 1/2... Step: 900... Loss: 0.646635... Val Loss: 0.662257\n",
      "Epoch: 1/2... Step: 1000... Loss: 0.753550... Val Loss: 0.662815\n",
      "Epoch: 1/2... Step: 1100... Loss: 0.646821... Val Loss: 0.629696\n",
      "Epoch: 1/2... Step: 1200... Loss: 0.645959... Val Loss: 0.646430\n",
      "Epoch: 1/2... Step: 1300... Loss: 0.635687... Val Loss: 0.635857\n",
      "Epoch: 1/2... Step: 1400... Loss: 0.680378... Val Loss: 0.632134\n",
      "Epoch: 1/2... Step: 1500... Loss: 0.585046... Val Loss: 0.591564\n",
      "Epoch: 1/2... Step: 1600... Loss: 0.682551... Val Loss: 0.594034\n",
      "Epoch: 1/2... Step: 1700... Loss: 0.723213... Val Loss: 0.585456\n",
      "Epoch: 1/2... Step: 1800... Loss: 0.526136... Val Loss: 0.586176\n",
      "Epoch: 1/2... Step: 1900... Loss: 0.512797... Val Loss: 0.570468\n",
      "Epoch: 1/2... Step: 2000... Loss: 0.705375... Val Loss: 0.614579\n",
      "Epoch: 1/2... Step: 2100... Loss: 0.560357... Val Loss: 0.563550\n",
      "Epoch: 1/2... Step: 2200... Loss: 0.598211... Val Loss: 0.565705\n",
      "Epoch: 1/2... Step: 2300... Loss: 0.624871... Val Loss: 0.570535\n",
      "Epoch: 1/2... Step: 2400... Loss: 0.651812... Val Loss: 0.548932\n",
      "Epoch: 1/2... Step: 2500... Loss: 0.534045... Val Loss: 0.561921\n",
      "Epoch: 1/2... Step: 2600... Loss: 0.642929... Val Loss: 0.586161\n",
      "Epoch: 1/2... Step: 2700... Loss: 0.497959... Val Loss: 0.553673\n",
      "Epoch: 1/2... Step: 2800... Loss: 0.415168... Val Loss: 0.549906\n",
      "Epoch: 1/2... Step: 2900... Loss: 0.631100... Val Loss: 0.545461\n",
      "Epoch: 1/2... Step: 3000... Loss: 0.503837... Val Loss: 0.546231\n",
      "Epoch: 1/2... Step: 3100... Loss: 0.551429... Val Loss: 0.538281\n",
      "Epoch: 1/2... Step: 3200... Loss: 0.604583... Val Loss: 0.563388\n",
      "Epoch: 1/2... Step: 3300... Loss: 0.522921... Val Loss: 0.528978\n",
      "Epoch: 1/2... Step: 3400... Loss: 0.450266... Val Loss: 0.535790\n",
      "Epoch: 1/2... Step: 3500... Loss: 0.359219... Val Loss: 0.607611\n",
      "Epoch: 1/2... Step: 3600... Loss: 0.573416... Val Loss: 0.558068\n",
      "Epoch: 1/2... Step: 3700... Loss: 0.529588... Val Loss: 0.529032\n",
      "Epoch: 1/2... Step: 3800... Loss: 0.552239... Val Loss: 0.540358\n",
      "Epoch: 1/2... Step: 3900... Loss: 0.625420... Val Loss: 0.591845\n",
      "Epoch: 1/2... Step: 4000... Loss: 0.541503... Val Loss: 0.522119\n",
      "Epoch: 1/2... Step: 4100... Loss: 0.578570... Val Loss: 0.526557\n",
      "Epoch: 1/2... Step: 4200... Loss: 0.437861... Val Loss: 0.509912\n",
      "Epoch: 1/2... Step: 4300... Loss: 0.450647... Val Loss: 0.512118\n",
      "Epoch: 1/2... Step: 4400... Loss: 0.485568... Val Loss: 0.513324\n",
      "Epoch: 1/2... Step: 4500... Loss: 0.498329... Val Loss: 0.532926\n",
      "Epoch: 1/2... Step: 4600... Loss: 0.526161... Val Loss: 0.507645\n",
      "Epoch: 1/2... Step: 4700... Loss: 0.392326... Val Loss: 0.514193\n",
      "Epoch: 1/2... Step: 4800... Loss: 0.500492... Val Loss: 0.522942\n",
      "Epoch: 1/2... Step: 4900... Loss: 0.547007... Val Loss: 0.517680\n",
      "Epoch: 1/2... Step: 5000... Loss: 0.392619... Val Loss: 0.503597\n",
      "Epoch: 1/2... Step: 5100... Loss: 0.459099... Val Loss: 0.520941\n",
      "Epoch: 1/2... Step: 5200... Loss: 0.515584... Val Loss: 0.523031\n",
      "Epoch: 1/2... Step: 5300... Loss: 0.526980... Val Loss: 0.518785\n",
      "Epoch: 1/2... Step: 5400... Loss: 0.493706... Val Loss: 0.512664\n",
      "Epoch: 1/2... Step: 5500... Loss: 0.557211... Val Loss: 0.507210\n",
      "Epoch: 1/2... Step: 5600... Loss: 0.650589... Val Loss: 0.494424\n",
      "Epoch: 1/2... Step: 5700... Loss: 0.437244... Val Loss: 0.505013\n",
      "Epoch: 1/2... Step: 5800... Loss: 0.588728... Val Loss: 0.518296\n",
      "Epoch: 1/2... Step: 5900... Loss: 0.488946... Val Loss: 0.503208\n",
      "Epoch: 1/2... Step: 6000... Loss: 0.554979... Val Loss: 0.543131\n",
      "Epoch: 1/2... Step: 6100... Loss: 0.690920... Val Loss: 0.492732\n",
      "Epoch: 1/2... Step: 6200... Loss: 0.570808... Val Loss: 0.503041\n",
      "Epoch: 1/2... Step: 6300... Loss: 0.291935... Val Loss: 0.501041\n",
      "Epoch: 1/2... Step: 6400... Loss: 0.481752... Val Loss: 0.492951\n",
      "Epoch: 1/2... Step: 6500... Loss: 0.524836... Val Loss: 0.505881\n",
      "Epoch: 1/2... Step: 6600... Loss: 0.668360... Val Loss: 0.515680\n",
      "Epoch: 1/2... Step: 6700... Loss: 0.800204... Val Loss: 0.516295\n",
      "Epoch: 1/2... Step: 6800... Loss: 0.438498... Val Loss: 0.504455\n",
      "Epoch: 1/2... Step: 6900... Loss: 0.765487... Val Loss: 0.490866\n",
      "Epoch: 1/2... Step: 7000... Loss: 0.516941... Val Loss: 0.487768\n",
      "Epoch: 1/2... Step: 7100... Loss: 0.580809... Val Loss: 0.501189\n",
      "Epoch: 1/2... Step: 7200... Loss: 0.536820... Val Loss: 0.491751\n",
      "Epoch: 1/2... Step: 7300... Loss: 0.352390... Val Loss: 0.498736\n",
      "Epoch: 1/2... Step: 7400... Loss: 0.721188... Val Loss: 0.518660\n",
      "Epoch: 1/2... Step: 7500... Loss: 0.406814... Val Loss: 0.486982\n",
      "Epoch: 1/2... Step: 7600... Loss: 0.462886... Val Loss: 0.502413\n",
      "Epoch: 1/2... Step: 7700... Loss: 0.430890... Val Loss: 0.494160\n",
      "Epoch: 1/2... Step: 7800... Loss: 0.323453... Val Loss: 0.497093\n",
      "Epoch: 1/2... Step: 7900... Loss: 0.629075... Val Loss: 0.496778\n",
      "Epoch: 1/2... Step: 8000... Loss: 0.629734... Val Loss: 0.493622\n",
      "Epoch: 1/2... Step: 8100... Loss: 0.350194... Val Loss: 0.497472\n",
      "Epoch: 1/2... Step: 8200... Loss: 0.441194... Val Loss: 0.498718\n",
      "Epoch: 1/2... Step: 8300... Loss: 0.635169... Val Loss: 0.494492\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9f5bf27b1e05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mval_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_h\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_on_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                   \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Training\n",
    "lr=0.0001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "epochs = 1\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip= 500 # gradient clipping\n",
    "\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "for e in range(epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        if counter%20 == 0:\n",
    "            lr = 0.1\n",
    "        else:\n",
    "            lr = 0.005\n",
    "        if counter>600 & counter%20 != 0:\n",
    "            lr = 0.0003\n",
    "        if counter>600 & counter%20 == 0:\n",
    "            lr = 0.003\n",
    "        if counter>1200 & counter%20 != 0:\n",
    "            lr = 0.0001\n",
    "        if counter>1200 & counter%20 == 0:\n",
    "            lr = 0.001      \n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=lr)      \n",
    "        h = tuple([each.data for each in h])\n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        if counter % print_every == 0:\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()     \n",
    "                try:\n",
    "                  output, val_h = net(inputs, val_h)\n",
    "                  val_loss = criterion(output.squeeze(), labels.float())\n",
    "                  val_losses.append(val_loss.item())\n",
    "                except:\n",
    "                  continue\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wzSHAW3pMl-b",
    "outputId": "7e931862-1a73-45d6-d3b0-c49544216af6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.488\n",
      "Test accuracy: 0.777\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = net.init_hidden(batch_size)\n",
    "net.eval()\n",
    "for inputs, labels in valid_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    try:    \n",
    "      output, h = net(inputs, h)\n",
    "      test_loss = criterion(output.squeeze(), labels.float())\n",
    "      test_losses.append(test_loss.item())\n",
    "    except:\n",
    "      continue\n",
    "    pred = torch.round(output.squeeze())\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(valid_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NLP project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
